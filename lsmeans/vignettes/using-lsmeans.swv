\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage[colorlinks=true, urlcolor=blue]{hyperref}
\usepackage{multicol}
\usepackage{mathpazo}
\usepackage{fancyvrb}

\def\lsmversion{1.10} %%% UPDATE THIS!

\title{Using the \texttt{lsmeans} Package}
\author{Russell V. Lenth\\The University of Iowa\\ 
\texttt{\href{mailto:russell-lenth@uiowa.edu}{russell-lenth@uiowa.edu}}}
\date{Updated with \code{lsmeans} Version~\lsmversion---\today}
%\VignetteIndexEntry{Using the lsmeans package}
%\VignetteDepends{lsmeans}
%\VignetteKeywords{least-squares means}
%\VignettePackage{lsmeans}


\DefineShortVerb{\"}
\DefineVerbatimEnvironment{Winput}{Verbatim}{fontshape=sl}
\DefineVerbatimEnvironment{Woutput}{Verbatim}{}
\def\bottomfraction{.5}
%\def\botfigrule{\hrule}
%\def\topfigrule{\hrule}
\def\code{\texttt}

\begin{Rcode}{hide,!echo}
library(lsmeans)
library(multcomp)
library(lme4)
options(width=88, digits=5)
\end{Rcode}



%%% NOTE This document is actually the .tex file generated by StatWeave %%%
%%%\usepackage{Statweave} %%% Needed stuff is already in preamble
\begin{document}
\maketitle{}

\section{What are least-squares means?}
\subsection{Introduction}
Least-squares means (or LS means), are generalizations of covariate-adjusted means, and date back at least to 1976 when they were incorporated in the contributed SAS procedure named \code{HARVEY} (Harvey 1976). Later, they were incorporated via \code{LSMEANS} statements in the regular SAS releases. SAS's documentation describes them as ``predicted population margins---that is, they estimate the marginal means over a balanced population'' (SAS Institute 2012).

People disagree on the appropriateness of LS~means. As in many statistical calculations, there are times when they are, and times when they are not. However, if one understands what is being calculated, one can judge its appropriateness. So the first task is to try to explain LS~means as clearly as possible. To that end, I offer this:
\begin{quote}\it
LS means are predictions from a model over a grid of predictor values; or marginal averages thereof. 
\end{quote}
More explicitly, define a set of \emph{reference levels}\footnote{I made up this term for convenience in explaining this stuff.} for each predictor, and create a grid consisting of all combinations of these. Make predictions on this grid, and (as needed), compute marginal means of those predictions, usually using equal weights.

The default in the "lsmeans" function is to set the reference levels as follows:
\begin{description}
\item[Factors] For predictors of class "factor" or "ordered", the default reference levels are the levels of the factor.
\item[Covariates] For numeric predictors, the default is to use a single reference level at the mean value of the predictor.
\end{description}

\subsection{Illustration}
To illustrate, consider the randomized block experiment given as an example in Box \emph{et al.} (2005), Table~4.4, page~146. In this experiment on penicillin manufacturing, five blocks (blends of material) were each tested with four treatments (variants of the process), and the process yield is measured.

To save space, I'll use just the data from the first three blends; and to make the example more interesting, suppose that a couple of the observations got lost. Let's enter the data and fit a model:
\begin{Rcode}
penicillin = expand.grid(treat = LETTERS[1:4], blend = factor(1:3))
penicillin$yield = c (
    89, 88, 97, 94,
    84, 77, NA, 79,
    NA, 87, 87, 85
)
penicillin.lm = lm(yield ~ treat + blend, data = penicillin)
\end{Rcode}
The reference levels are simply the factor levels.
Here are the LS~means for the factor combinations (as specified by "~ treat * blend" in the call):
\begin{Rcode}{saveout}
library(lsmeans)
lsmeans(penicillin.lm, ~ treat * blend)
\end{Rcode}
\begin{multicols}{2}\small
\recallout{lastchunk}
\end{multicols}

One can verify that these are simply the predicted values from the model for all 12 factor combinations (including those where there are missing values):
\begin{Rcode}
predict(penicillin.lm, newdata = penicillin)
\end{Rcode}
The LS~means for "treat" are simply the marginal averages of these values over the five blends:
\begin{Rcode}
lsmeans(penicillin.lm, ~ treat)
\end{Rcode}
For treatments $B$ and~$D$, these LS~means are the same as the marginal means of the data; but for treatments $A$ and~$C$, where missing values occur, they are not the data means, but instead they are model-based predictions of those marginal means. This is an example where I believe most would think these LS~means are a reasonable way to summarize the model results.




\section{Analysis-of-covariance example}
Oehlert (2000), p.456 gives a dataset concerning repetitive-motion pain due to typing on three types of ergonomic keyboards. Twelve subjects having repetitive-motion disorders were randomized to the keyboard types, and reported the severity of their pain on a subjective scale of 0--100 after two weeks of using the keyboard. We also recorded the time spent typing, in hours. Here we enter the data, and obtain the plot shown in Figure~\ref{typing-fig}.
\begin{Rcode}{fig, height=3.75, width=8, scale=.75, label=typing-scatter, savefig}
typing = data.frame(
  type = rep(c("A","B","C"), each=4),
  hours = c(60,72,61,50, 54,68,66,59, 56,56,55,51),
  pain =  c(85,95,69,58, 41,74,71,52, 41,34,50,40))
library(lattice)
xyplot(pain ~ hours | type, data = typing, layout = c(3,1))
\end{Rcode}
\begin{figure}
\recallfig{typing-scatter}
\caption{Display of the keyboard-pain data.}\label{typing-fig}
\end{figure}


It appears that "hours" and "pain" are linearly related (though it's hard to know for type~$C$ keyboards), and that the trend line for type~$A$ is higher than for the other two. To test this,  consider a simple covariate model that fits parallel lines to the three panels:
\begin{Rcode}
typing.lm = lm(pain ~ hours + type, data = typing)
\end{Rcode}
As mentioned above, the reference levels for "type" are the three keyboard types, whereas the reference levels for "hours" is the mean value of "hours" over the whole dataset:
\begin{Rcode}{saveout}
lsmeans(typing.lm, list(~ type, ~ type * hours))
\end{Rcode}
\begin{multicols}{2}\small
\recallout{lastchunk}
\end{multicols}
The second table shows explicitly that only one reference value is used for the covariate, "hours", hence each table has the same LS~means. These results are the same as what are often called ``adjusted means'' in the analysis of covariance---predicted values for each keyboard type, when the covariate is set to its overall average value.

We can use the "at" argument to override the default reference grid. For example, suppose we want to consider "hours" values of $(55,59,64)$:
\begin{Rcode}
lsmeans(typing.lm, list(~ type * hours, ~ type, ~ hours),
    at = list(hours = c(55,59,64)))
\end{Rcode}
The first set of LS~means are the same as before when "hours" equals 59. But the marginal LS~means for "type" are different from those before because we have averaged over the predictions for three different "hours" values. This is an example where the marginal LS~means for "type" probably \emph{don't} make a lot of sense, unless there is a really good reason for picking those three particular "hours" values. On the other hand, the LS~means for "hours" do make sense, as they represent the average of the predictions for all three keyboard types.


\section{Contrasts and comparisons}
Often, we want to perform multiple comparisons or contrasts among a set of LS~means. "lsmeans" provides for this by specifying something on the left-hand side of the formula. For example, in the keyboard-pain example, we can obtain pairwise comparisons among the adjusted means as follows:
\begin{Rcode}
lsmeans(typing.lm, pairwise ~ type)
\end{Rcode}
Note that "lsmeans" produces two tables for ach two-sided formula---the first is the LS~means, and the second is the contrast output. 

There are other choices besides "pairwise". The other built-in options are "revpairwise" (same as "pairwise" but the subtraction is done the other way; "trt.vs.ctrl" for comparing one factor level (say, a control) with each of the others, and the related "trt.vs.ctrl1", and "trt.vs.ctrlk" for convenience in specifying which group is the control group; "poly" for estimating orthogonal-polynomial contrasts, assuming equal spacing; and "effects" and "del.effects", which compare each LS~mean with the average of all (or all others). It is possible to provide custom contrasts as well---see the documentation.

As seen in the previous output, "lsmeans" provides for adjusting the $p$~values of contrasts to preserve a familywise error rate. The default for pairwise comparisons is the Tukey (HSD) method. One must use these adjustments with caution. For example, when the standard errors are unequal, the Tukey method is only approximate, even under normality and independence assumptions. 
To get a more exact adjustment, we can pass the comparisons to the "glht" function in the "multcomp" package (and also pass additional arguments---in the coming example, none). Then the returned value for the contrasts is a "glht" object instead of a "data.frame":
\begin{Rcode}
library(multcomp)
typing.lsm = lsmeans(typing.lm, pairwise ~ type, glhargs=list())
typing.lsm[[2]]
\end{Rcode}
These $p$~values are exact (if the assumptions hold) and, as expected, slightly different from those in the previous "lsmeans" output. We may of course use other methods available for "glht" objects. The plot below displays the comparisons in the preceding tabls:
\begin{Rcode}{fig, height=3in, scale=.6, label=typing-glht-plot}
plot(typing.lsm[[2]])
\end{Rcode}

Besides being able to call "glht" from "lsmeans", we have also provided an "lsm" function and an associated "glht" method so that we can call "lsmeans" from within "glht". We use "lsm" in much the same way as "mcp" in the "multcomp" package. Here we display simultaneous confidence intervals for the LS~means:
\begin{Rcode}{fig, height=3in, scale=.6, label=typing-confint-plot}
typing.glht = glht(typing.lm, linfct = lsm(~ type))
plot(typing.glht)
\end{Rcode}
\\
Unlike "lsmeans" which returns a list, the design of "lsm" is to create just one set of linear functions to hand to "glht". It returns contrast output if available, otherwise LS~means output; so In the illustration above, the linear functions of the lsmeans themselves are used. If we had instead specified "lsm(pairwise ~ type)", then the results would have been the same as shown earlier for the pairwise differences. 


\section{Two-factor example}
Now consider the R-provided dataset "warpbreaks", relating to a weaving-process experiment. This dataset (from Tukey 1977, p.82) has two factors: "wool" (two types of wool), and "tension" (low, medium, and high); and the response variable is "breaks", the nuumber of breaks in a fixed length of yarn.
%To make it more interesting, we'll delete some cases so that the design is unbalanced.
% \begin{Rcode}
% warp = warpbreaks[-c(1,2,3,5,8,13,21,34), ]
% with(warp, table(wool, tension))
% \end{Rcode}
\begin{Rcode}
with(warpbreaks, table(wool, tension))
\end{Rcode}
Let us fit a model that includes interaction
\begin{Rcode}
warp.lm = lm(breaks ~ wool * tension, data = warpbreaks)
anova(warp.lm)
\end{Rcode}
The "lsmeans" package provides a function "lsmip" that provides an interaction plot based on the LS~means:
\begin{Rcode}{fig, scale=.5, height=4.25in, label=warp-intplot}
lsmip(warp.lm, wool ~ tension)
\end{Rcode}
\\
Now we can obtain the least-squares means for the "wool"$\times$"tension" combinations. We could request pairwise comparisons as well by specifying "pairwise ~ wool:tension", but this will yield quite a few comparisons (15 to be exact). Often, people are satisfied with a smaller number of comparisons (or contrasts) obtained by restricting them to be at the same level of one of the factors. This can be done using the "|" symbol for conditioning. In the code below, we request comparisons of the wools at each tension, and polynomial contrasts for each wool.
\begin{Rcode}
print(lsmeans(warp.lm, list(pairwise ~ wool | tension,  poly ~ tension | wool)), omit=3)
\end{Rcode}
(We suppressed the third element of the results because it is the same as the first, with rows rearranged.)
With these data, the least-squares means are exactly equal to the cell means of the data.
The main result (visually clear in the interaction plot) is that the wools differ the most when the tension is low. The signs of the polynomial contrasts indicate decrasing trends for both wools, but opposite concavities.

It is also possible to abuse "lsmeans" with a call like this:
\begin{Rcode}
lsmeans(warp.lm, ~ wool)   ### NOT a good idea!
\end{Rcode}
Each lsmean is the average of the three "tension" lsmeans at the given "wool". As the warning indicates, the presence of the strong interaction indicates that these results are pretty meaningless. In another dataset where an additive model would explain the data, these marginal averages, and comparisons or contrasts thereof, can nicely summarize the main effects in an interpretable way.

\section{Split-plot example}
The "nlme" package includes a famous dataset "Oats" that was used in Yates~(1935) as an example of a split-plot experiment. Here is a summary of the dataset.
\begin{Rcode}
data(Oats, package="nlme")
summary(Oats)
\end{Rcode}
The experiment was conducted in six blocks, and each block was divided into three plots, which were randomly assigned to varieties of oats. With just "Variety" as a factor, it is a randomized complete-block experiment. However, each plot was subdivided into 4 subplots and the subplots were treated with different amounts of nitrogen. Thus, "Block" is a blocking factor, "Variety" is the whole-plot factor, and "nitro" is the split-plot factor. The response variable is "yield", the yield of each subplot in bushels per~acre. Below is an interaction plot of these data.
\begin{Rcode}{fig, scale=.5, height=4, label=oats-intplot}
\coderef{hidden}{par(mar=c(4,4,1,.1))}
with(Oats, interaction.plot(nitro, Variety, yield, type="b"))
\end{Rcode}
%\hfill
%\begin{Rcode}{fig, !echo, hide, scale=.5, height=4, label=oats-intplot2}
%par(mar=c(4,4,1,.1))
%library(lme4)
%Oats.lmer = lmer(yield ~ Variety + factor(nitro) + (1 | Block/Variety), data=Oats)
%Oats.lsms = lsmeans(Oats.lmer, ~ nitro:Variety)
%with(Oats.lsms[[1]], interaction.plot(nitro, Variety, lsmean, type="b", ylab="Least-Squares means"))
%\end{Rcode}
\\
There is not much evidence of an interaction. In this dataset, we have random factors "Block" and "Block:Variety" (which identifies the plots). So we will fit a linear mixed-effects model that accounts for these. Another technicality is that "nitro" is a numeric variable, and initially we will model it as a factor. We will use "lmer" in the "lme4" package to fit a model, and display the marginal LS~means with appropriate contrasts.
\enlargethispage{-36pt} %%% MANUAL FORMATTING %%%
\begin{Rcode}
library(lme4)
Oats.lmer = lmer(yield ~ Variety + factor(nitro) + (1 | Block/Variety), data=Oats)
lsmeans(Oats.lmer, list(revpairwise ~ Variety,  poly ~ nitro))
\end{Rcode}
The polynomial contrasts for "nitro" suggest that we could substitute a quadratic trend for "nitro"; and if we do that, then there is another (probably better) way to make the above predictions:
\begin{Rcode}
OatsPoly.lmer = lmer(yield ~ Variety + poly(nitro, 2) + (1 | Block/Variety), data=Oats)
\end{Rcode}
The graphs below show the LS~means from these two models.
\begin{multicols}{2}
\begin{Rcode}{fig, scale=.5, height=4, label=oat-add-lsmip}
lsmip(Oats.lmer, Variety ~ nitro)
\end{Rcode}
\begin{Rcode}{fig, scale=.5, height=4, label=oat-poly-lsmip}
lsmip(OatsPoly.lmer, Variety ~ nitro, 
    cov.reduce = FALSE)
\end{Rcode}
\end{multicols}
\noindent
These plots are nearly identical. The "lsmip" function works by calling "lsmeans" with a specification for the required factor combinations. In the second plot, we passed the extra argument "cov.reduce = FALSE" to "lsmeans", which causes it to use the unique values of "nitro" rather than predicting at the average of "nitro".



\section{Messy data}
To illustrate some more issues, and related \code{lsmeans} capabilities, consider the dataset named \code{nutrition} that is provided with the \code{lsmeans} package. These data come from Milliken and Johnson (1984), and contain the results of an observational study on nutrition education. Low-income mothers are classified by race, age category, and whether or not they received food stamps (the \code{group} factor); and the response variable is a gain score (post minus pre scores) after completing a nutrition training program. The graph below displays the data.
\begin{Rcode}{fig, width=9in, scale=.5,label=nutr-scatter}
xyplot(gain ~ age | race*group, data=nutrition)
\end{Rcode}

Consider the model that includes all main effects and two-way interactions; and let us look at the \code{group} by \code{race} lsmeans:
\begin{Rcode}
nutr.lm = lm(gain ~ (age + group + race)^2, data = nutrition)
lsmeans(nutr.lm, ~ group*race)
\end{Rcode}
One thing that this illustrates is that \code{lsmeans} incorporates an estimability check, and returns a missing value when a prediction cannot be made uniquely. In this example, we have very few Hispanic mothers in the dataset, resulting in empty cells. This creates a rank deficiency in the fitted model and some predictors are thrown out.

The "lsmip" function can display a three-way interaction plot

\begin{Rcode}{fig, height=3.25in, scale=.75,label=nutr-intplot}
lsmip(nutr.lm, race ~ age | group)
\end{Rcode}


We can avoid non-estimable cases by using "at" to restrict the reference levels to a smaller set:
\begin{Rcode}
lsmeans(nutr.lm, ~ group*race, at = list(age = "3"))
\end{Rcode}
Nonetheless, the standard errors for the Hispanic mothers are enormous due to very small counts.
One useful summary of the results is to narrow the scoe of the reference levels to two races and the two middle age groups, where most of the data lie. Here are the lsmeans and comparisons within rows and columns
\begin{Rcode}
nutr.lsm = lsmeans(nutr.lm, list(pairwise~group|race, pairwise~race|group),
        at = list(age=c("2","3"), race=c("Black","White")))
nutr.lsm[-3]
\end{Rcode}
The general conclusion from these analyses is that (except for age 4, where the data are very sparse), the expected gains from the training are higher among families receiving food stamps.
Note that this analysis is somewhat different than the results we would obtain by subsetting the data, as we are borrowing information from the other observations in estimating and testing these LS~means.


%\clearpage %-------- MANUAL FORMATTING ---------
\section{GLMM example}
The dataset "cbpp" in the "lme4" package, originally from Lesnoff \emph{et al.}~(1964), provides data on the incidence of contagious bovine pleuropneumonia in 15 herds of zebu cattle in Ethiopia, collected over four time periods. These data are used as the primary example for the "glmer" function, and it is found that a model that accounts for overdisperion is advantageous; hence the addition of the "(1|obs)" in the model fitted below. 

"lsmeans" may be used as in linear models to obtain marginal linear predictions for a generalized linear model or, in this case, a generalized linear mixed model. Here, we use the "trt.vs.ctrl1" contrast family to compare each period with the first, as the primary goal was to track the spread or decline of CBPP over time.
\begin{Rcode}
cbpp$obs = 1:nrow(cbpp)
cbpp.glmer = glmer(cbind(incidence, size - incidence) 
   ~ period + (1 | herd) + (1 | obs),  family = binomial,  data = cbpp)
anova(cbpp.glmer)   
\end{Rcode}
We will save the results from "lsmean", then add the inverse logits of the predictions and the estimated odds ratios for the comparisons as an aid in interpretation.
\begin{Rcode}
cbpp.lsm = lsmeans(cbpp.glmer, trt.vs.ctrl1 ~ period)
cbpp.lsm[[1]]$pred.incidence = 1 - 1 / (1 + exp(cbpp.lsm[[1]]$lsmean))
cbpp.lsm[[2]]$odds.ratio = exp(cbpp.lsm[[2]]$estimate)
cbpp.lsm
\end{Rcode}
When degrees of freedom are not available, as in this case, "lsmeans" emphasizes that fact by displaying "NA" for degrees of freedom and in the column headings.
%In a way, the comparisons table is not needed because the results are the same as the regression coefficients under the default parameterization.


\section{Trends}
The "lsmeans" function also provides for estimating and comparing the slopes of fitted lines (or curves). To illustrate, consider the built-in R dataset "ChickWeight" which has data on the growths of newly hatched chicks under four different diets. Here is a display of the dataset.
%The dataset is displayed in Figure~\ref{chick-fig}.
%\begin{figure}
\begin{Rcode}{fig, height=3, width=8, scale=.8, label=chick-plot}
xyplot(weight~Time | Diet, groups = ~ Chick, data=ChickWeight, type="o", layout=c(4,1))
\end{Rcode}
%\caption{Growth curves of chicks, dataset \texttt{ChickWeight}.}\label{chick-fig}
%\end{figure}

Let us fit a model to these data using random slopes for each chick and allowing for a different average slope for each diet:
\begin{Rcode}
Chick.lmer = lmer(weight ~ Diet * Time + (0 + Time | Chick), data = ChickWeight)
print(Chick.lmer, corr = FALSE)
\end{Rcode}
Then call "lsmeans" with the "trend" argument to estimate and compare the average slopes for each diet:
\begin{Rcode}
lsmeans(Chick.lmer, revpairwise ~ Diet, trend = "Time")
\end{Rcode}
The tests of comparisons with Diet~1 match those from the regression coefficients, as they should.


\section{Contrasts}
You may occasionally want to know exactly what contrast coefficients are being used, especially in the polynomial case. Contrasts are implemented in functions having names of the form \texttt{\textsl{name}.lsmc} (``lsmc'' for ``least-squares means contrasts''), and you can simply call that function to see the contrasts; for example,
\begin{Rcode}
poly.lsmc(1:4)
\end{Rcode}
"poly.lsmc" uses the base function "poly" plus an \emph{ad hoc} algorithm that tries (and usually succeeds) to make integer coefficients, copmparable to what you find in published tables of orthogonal polynomial contrasts.

You may supply your own custom contrasts in two ways. One is to supply a "contr" argument in the "lsmeans" call, like this:
\begin{Rcode}
print(lsmeans(typing.lm, custom.comp ~ type, 
              contr = list(custom.comp = list(fancy.contrast=c(1, -.75, -.25)))), 
    omit=1)
\end{Rcode}
Each contrast family is potentially a list of several contrasts, and there are potentially more than one contrast family; so we must provide a list of lists.

The other way is to create your own ".lsmc" function, and use its base name in a formula:
\begin{Rcode}
inward.lsmc = function(levs, ...) {
    n = length(levs)
    result = data.frame(`grand mean` = rep(1/n, n))
    for (i in 1 : floor(n/2)) {
        x = rep(0, n)
        x[1:i] = 1/i
        x[(n-i+1):n]  = -1/i
        result[[paste("first", i, "vs last", i)]] = x
    }
    attr(result, "desc") = "grand mean and inward contrasts"
    attr(result, "adjust") = "none"
    result
}
\end{Rcode}
Testing it, we have
\begin{Rcode}
inward.lsmc(1:5)
\end{Rcode}
\ldots\ and an application:
\begin{Rcode}
print(lsmeans(Oats.lmer, inward ~ nitro), omit=1)
\end{Rcode}


\section{Differences from SAS}
"lsmeans" started out with a goal of providing similar capabilities to the "LSMEANS" statements in various SAS procedures. The points below do not list all differences from SAS, but may help you understand how they differ and navigate how to translate a SAS specification to an "lsmeans" one.
\begin{itemize}
\item SAS will not print LS~means for factor combinations unless the model contains a corresponding interaction term.
\item SAS allows only factors (i.e., CLASS variables) in the specification of levels for LS~means. The "lsmeans" function allows covariates as well.
\item SAS does not seem to allow multiple "at" values for a covariate.
\item As I understand it, SAS's "OBSMARGINS" ("OM") option allows one to specify a dataset that defines a grid of reference levels. In the R "lsmeans" function, this is done more simply using "at" (or in one special case,  "cov.reduce=FALSE").
\item For unequal weights for the marginal LS~means, in SAS one must construct the "OM" dataset to reflect the desired proportions, or has a "weight" variable; whereas in "lsmeans" we customize the "fac.reduce" function.
\item Some of the capabilities of SAS's "split" and "bylevel" options are provided by using a conditioning symbol ``"|"'' in the "lsmeans" specification to delineate the desired slices. "lsmeans" does not output $F$~tests for the slices.
\end{itemize}

\ifx %-------------------- EXCISED ------------------
In general, now that covariates can be specified in an LS~means specification, the conceptual framework of "lsmeans" has evolved to serve a more general purpose than SAS does of providing predictions for any linear model on a grid of reference levels. For example, suppose that we have a model "mod" that involves a fractor "treat" and a complicated functions of two numeric variables "x1" and "x2". We can use "lsmeans" to easily create the data for side-by-side contour plots of the fitted surface, via code like this:
\begin{Rcode}{!eval}
require(lattice)
contourplot(lsmean ~ x1*x2|treat, 
    data = lsmeans(mod, ~x1*x2|treat, at = list(x1=x1.vals, x2=x2.vals))[[1]])
\end{Rcode}
where "x1.vals" and "x2.vals" define the grid of values for the contour plot.
\fi %-------------------- END OF CUT ---------------------

\section*{References}
\begin{description}

\item[Box, G., Hunter,, J., and Hunter, W.~(2005)]
\emph{Statistics for Experimenters} (2nd ed.).
Wiley.

\item[Harvey, W.~(1976)] Use of the HARVEY procedure.
SUGI Proceedings, 
\url{http://www.sascommunity.org/sugi/SUGI76/Sugi-76-20%20Harvey.pdf} (accessed June 20, 2013).

\item[Lesnoff, M., Laval, G., Bonnet, P., \emph{et al.}~(2004)] Within-herd spread of contagious bovine pleuropneumonia in Ethiopian highlands, \emph{Preventive Veterinary Medicine},  \textbf{64}, 27--40.

\item[Milliken, G. A. and Johnson, D. E. (1984)]
\emph{Analysis of Messy Data -- Volume I: Designed Experiments}. Van Nostrand, ISBN 0-534-02713-7.


\item[Oehlert, G. (2000)] \emph{A First Course in Design and Analysis of Experiments}, W.~H.~Freeman.
This is out-of-print, but now available under a Creative Commons license via \url{http://users.stat.umn.edu/~gary/Book.html} (accessed August 23, 2012).

\item[SAS Institute Inc. (2012)]
Online documentation, SAS/STAT version 9.3: Shared concepts: LSMEANS statement.
\url{http://support.sas.com/documentation/cdl/en/statug/63962/HTML/default/viewer.htm#statug_introcom_a0000003362.htm} (accessed August 14, 2012).

\item[Tukey, J.~W.~(1977)] \emph{Exploratory Data Analysis}. Addison-Wesley. 

\item[Yates, F. (1935)] Complex experiments, \emph{Journal of the Royal Statistical Society} (Supplement), \textbf{2}, 181--247.

\end{description}


\end{document}


%%% Complete penicillin dataset
penicillin = expand.grid(treat = LETTERS[1:4], blend = factor(1:5))
penicillin$yield = c (
    89, 88, 97, 94,
    84, 77, 92, 79,
    81, 87, 87, 85,
    87, 92, 89, 84,
    79, 81, 80, 88
)
penicillin.lm = lm(yield ~ blend + treat, data = penicillin)

