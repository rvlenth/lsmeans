\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage[colorlinks=true, urlcolor=blue]{hyperref}
\usepackage{multicol}
\usepackage{mathpazo}
\usepackage{fancyvrb}

\def\lsmversion{1.10} %%% UPDATE THIS!

\title{Using the \texttt{lsmeans} Package}
\author{Russell V. Lenth\\The University of Iowa\\ 
\texttt{\href{mailto:russell-lenth@uiowa.edu}{russell-lenth@uiowa.edu}}}
\date{Updated with \code{lsmeans} Version~\lsmversion---\today}
%\VignetteIndexEntry{Using the lsmeans package}
%\VignetteDepends{lsmeans}
%\VignetteKeywords{least-squares means}
%\VignettePackage{lsmeans}


\DefineShortVerb{\"}
\DefineVerbatimEnvironment{Winput}{Verbatim}{fontshape=sl}
\DefineVerbatimEnvironment{Woutput}{Verbatim}{}
\def\bottomfraction{.5}
\def\botfigrule{\hrule}
\def\topfigrule{\hrule}
\def\code{\texttt}

\begin{Rcode}{hide,!echo}
library(multcomp)
\end{Rcode}



%%% NOTE This document is actually the .tex file generated by StatWeave %%%
%%%\usepackage{Statweave} %%% Needed stuff is already in preamble
\begin{document}
\maketitle{}

\section{What are least-squares means?}
\subsection{Introduction}
Least-squares means (or LS means), are generalizations of covariate-adjusted means, and date back at least to 1976 when they were incorporated in the contributed SAS procedure named \code{HARVEY} (Harvey 1976). Later, they were incorporated via \code{LSMEANS} statements in the regular SAS releases. SAS's documentation describes them as ``predicted population margins---that is, they estimate the marginal means over a balanced population'' (SAS Institute 2012).

People disagree on the appropriateness of LS~means. As in many statistical calculations, thet are appropriate only sometimes, and if one understands what is being calculated, one can judge its appropriateness. So the first task is to try to explain it more explicitly than SAS's documentation. To that end, I will say this:
\begin{quote}\it
LS means are predictions from a model, or marginal averages thereof. 
\end{quote}
Certainly there is nothing controversial about prediction, as long as there are no identifiability issues and one understands the independent variables' values at which the prediction is made. 

So the trouble lies in the marginal averaging of predictions. To understand this thoroughly, I find it helpful to consider what I will term the \emph{reference levels} of the independent variables in a model. These consist of the value, or values, to consider for each independent variable. Let us form a grid consisting of all combinations of the reference levels, and use the model to predict the response at each grid value. Then the individual predictions are LS~means of the reference-level combinations. To obtain the LS~means for some subset of the predictors, simply compute the corresponding marginal averages of this table of predictions. Typically (as is the default in the "lsmeans" function as well as in SAS), grid values are given equal weight when averaging; but weighted averages are possible if deemed more appropriate.

\subsection{Illustration}
To illustrate, consider the randomized block experiment given as an example in Box \emph{et al.} (2005), Table~4.4, page~146. In this experiment on penicillin manufacturing, five blocks (blends of material) were each tested with four treatments (variants of the process), and the process yield is measured. Here we enter the data and fit a model
\begin{Rcode}
penicillin = expand.grid(treat = LETTERS[1:4], blend = factor(1:5))
penicillin$yield = c (
    89, 88, 97, 94,
    84, 77, 92, 79,
    81, 87, 87, 85,
    87, 92, 89, 84,
    79, 81, 80, 88
)
penicillin.lm = lm(yield ~ blend + treat, data = penicillin)
\end{Rcode}
Now consider the reference levels consisting of the five blends and four treatments. Then the LS means for the combinations of these factors are simply the predictions from the model:\footnote{Interestingly, SAS won't output these results because the model does not contain the interaction term.}
\begin{Rcode}
(pred.yield = matrix(predict(penicillin.lm), nrow=5, byrow=TRUE))
\end{Rcode}
The LS~means for blends and for treatments are simply the row and column averages, respectively:
\begin{Rcode}
apply(pred.yield, 1, mean)
\end{Rcode}
\begin{Rcode}
apply(pred.yield, 2, mean)
\end{Rcode}
Due to the linearity of everything, these are the same as the row and column averages of the original "yield" values; so we have not broken much new ground yet.

However, suppose that something had gone wrong (for reasons unrelated to the experimental conditions) so that a couple of data values are missing:
\begin{Rcode}
badpen = penicillin
badpen$yield[7] = badpen$yield[17] = NA
badpen.lm = update(penicillin.lm, data = badpen)
\end{Rcode}
With the additive model, we still can obtain predictions for the grid of reference values:
\begin{Rcode}
(pred.yield.bad = matrix(predict(badpen.lm, new=penicillin), nrow=5, byrow=TRUE))
\end{Rcode}
So now the LS~means for treatments are
\begin{Rcode}
apply(pred.yield.bad, 2, mean)
\end{Rcode}
The LS~means for treatments $B$ and $D$, where we have complete data, are unchanged from those obtained earlier. The other two are based on estimating the missing cells based on the additive model. This is an example where I think most would agree that it is appropriate to summarize the results for treatments using equally-weighted marginal means of predictions; so LS~means serve a useful purpose here.

The "lsmeans" function computes these results using a formula interface, and obtains standard errors and confidence intervals from the estimated covariance matrix of the opredictions:
\begin{Rcode}
library(lsmeans)
lsmeans(badpen.lm, list(~ treat, ~ blend))
\end{Rcode}

\subsection{Default reference levels}


\section{Analysis-of-covariance example}
Oehlert (2000), p.456 gives a dataset concerning repetitive-motion pain due to typing on three types of ergonomic keyboards. Twelve subjects having repetitive-motion disorders were randomized to the keyboard types, and reported the severity of their pain on a subjective scale of 0--100 after two weeks of using the keyboard. We also recorded the time spent typing, in hours. Here are the data, and a plot.
\begin{Rcode}{fig, height=4.5, width=12, scale=.5, label=typing-scatter}
typing = data.frame(
  type = rep(c("A","B","C"), each=4),
  hours = c(60,72,61,50, 54,68,66,59, 56,56,55,51),
  pain =  c(85,95,69,58, 41,74,71,52, 41,34,50,40))
library(lattice)
xyplot(pain ~ hours | type, data = typing, layout = c(3,1))
\end{Rcode}

It appears that "hours" and "pain" are linearly related (though it's hard to know for type~$C$ keyboards), and that the trend line for type~$A$ is higher than for the other two. To test this,  consider a simple covariate model that fits parallel lines to the three panels:
\begin{Rcode}
typing.lm = lm(pain ~ hours + type, data = typing)
\end{Rcode}
The least-squares means resulting from this model are easily obtained by calling "lsmeans" with the fitted model and a formula specifying the factor of interest:
\begin{Rcode}
lsmeans(typing.lm, ~ type)
\end{Rcode}
These results are the same as what are often called ``adjusted means'' in the analysis of covariance---predicted values for each keyboard type, when the covariate is set to its overall average value, as we now verify:
\begin{Rcode}
predict(typing.lm, newdata = data.frame(type = c("A","B","C"), 
    hours = mean(typing$hours)))
\end{Rcode}
The "lsmeans" function allows us to make predictions at other "hours" values. We may also obtain comparisons or contrasts among the means by specifying a keyword in the left-hand side of the formula. For example,
\begin{Rcode}
lsmeans(typing.lm, pairwise ~ type, at = list(hours = 55))
\end{Rcode}
The resulting least-squares means are each about $7.3$ less than the previous results, but their standard errors don't all change the same way: the first two SEs increase but the third decreases because the prediction is closer to the data in that group.

The results for the pairwise differences are the same regardless of the "hours" value we specify, because the "hours" effect cancels out when we take the differences. We confirm that the mean pain with keyboard~$A$ is significantly greater than it is with either of the other keyboards.

There are other choices besides "pairwise". The other built-in options are "revpairwise" (same as "pairwise" but the subraction is done the other way; "trt.vs.ctrl" for comparing one factor level (say, a control) with each of the others, and the related "trt.vs.ctrl1", and "trt.vs.ctrlk" for convenience in specifying which group is the control group; and "poly" for estimating orthogonal-polynomial contrasts, assuming equal spacing. It is possible to provide custom contrasts as well---see the documentation.

As seen in the previous output, "lsmeans" provides for adjusting the $p$~values of contrasts to preserve a familywise error rate. The default for pairwise comparisons is the Tukey (HSD) method. But in covariance models, that method is only approximate. To get a more exact adjustment, we can pass the comparisons to the "glht" function in the "multcomp" package (and also pass additional arguments---in this example, none):
\begin{Rcode}
typing.lsm = lsmeans(typing.lm, pairwise ~ type, glhargs=list())
print(typing.lsm, omit=1)
\end{Rcode}
The $p$~values are slightly different, as expected. We may of course use other methods available for "glht" objects:
\begin{Rcode}{fig, height=3in, scale=.6, label=typing-glht-plot}
plot(typing.lsm[[2]])
\end{Rcode}

Besides being able to call "glht" from "lsmeans", we have also provided an "lsm" function and an associated "glht" method so that we can call "lsmeans" from withing "glht". We use "lsm" in much the same way as "mcp" in the "multcomp" package. Here we display simultaneous confidence intervals for the lsmeans:
\begin{Rcode}{fig, height=3in, scale=.6, label=typing-confint-plot}
typing.glht = glht(typing.lm, linfct = lsm(~ type))
plot(typing.glht)
\end{Rcode}
\\
\clearpage %%% MANUAL FORMATTING %%%
Unlike "lsmeans" which returns a list, the design of "lsm" is to create just one set of linear functions to hand to "glht". In the illustration above, there is no left-hand side of the formula, so the linear functions of the lsmeans themselves are used. If we had instead specified "lsm(pairwise ~ type)", then the results would have been the same as shown earlier for the pairwise differences. 


\section{Two-factor example}
Now consider the R-provided dataset "warpbreaks", relating to a weaving-process experiment. This dataset (from Tukey 1977, p.82) has two factors: "wool" (two types of wool), and "tension" (low, medium, and high); and the response variable is "breaks", the nuumber of breaks in a fixed length of yarn.
%To make it more interesting, we'll delete some cases so that the design is unbalanced.
% \begin{Rcode}
% warp = warpbreaks[-c(1,2,3,5,8,13,21,34), ]
% with(warp, table(wool, tension))
% \end{Rcode}
\begin{Rcode}
with(warpbreaks, table(wool, tension))
\end{Rcode}
An interaction plot clearly indicates that we shouldn't consider an additive model.
\begin{Rcode}{fig, scale=.5, height=4.25in, label=warp-intplot}
\coderef{hidden}{par(mar=c(4,4,1,.1))}
with(warpbreaks, interaction.plot(tension, wool, breaks, type="b"))
\end{Rcode}
\\
So let us fit a model with interaction
\begin{Rcode}
warp.lm = lm(breaks ~ wool * tension, data = warpbreaks)
anova(warp.lm)
\end{Rcode}
Now we can obtain the least-squares means for the "wool"$\times$"tension" combinations. We could request pairwise comparisons as well by specifying "pairwise ~ wool:tension", but this will yield quite a few comparisons (15 to be exact). Often, people are satisfied with a smaller number of comparisons (or contrasts) obtained by restricting them to be at the same level of one of the factors. This can be done using the "|" symbol for conditioning. In the code below, we request comparisons of the wools at each tension, and polynomial contrasts for each wool.
\begin{Rcode}
print(lsmeans(warp.lm, list(pairwise ~ wool | tension,  poly ~ tension | wool)), omit=3)
\end{Rcode}
(We suppressed the third element of the results because it is the same as the first, with rows rearranged.)
With these data, the least-squares means are exactly equal to the cell means of the data.
The main result (visually clear in the interaction plot) is that the wools differ the most when the tension is low. The signs of the polynomial contrasts indicate decrasing trends for both wools, but opposite concavities.

It is also possible to abuse "lsmeans" with a call like this:
\begin{Rcode}
lsmeans(warp.lm, ~ wool)   ### NOT a good idea!
\end{Rcode}
Each lsmean is the average of the three "tension" lsmeans at the given "wool". As the warning indicates, the presence of the strong interaction indicates that these results are pretty meaningless. In another dataset wher an additive model would explain the data, these marginal averages, and comparisons or contrasts thereof, can nicely summarize the main effects in an interpretable way.

\section{Split-plot example}
The "nlme" package includes a famous dataset "Oats" that was used in Yates~(1935) as an example of a split-plot experiment. Here is a summary of the dataset.
\begin{Rcode}{saveout}
library(nlme)
summary(Oats)
\end{Rcode}
\recallout{lastchunk}
The experiment was conducted in six blocks, and each block was divided into three plots, which were randomly assigned to varieties of oats. With just "Variety" as a factor, it is a randomized complete-block experiment. However, each plot was subdivided into 4 subplots and the subplots were treated with different amounts of nitrogen. Thus, "Block" is a blocking factor, "Variety" is the whole-plot factor, and "nitro" is the split-plot factor. The response variable is "yield", the yield of each subplot in bushels per~acre. Below is an interaction plot of the data, and also an interaction pl;ot of the least-squares means, which will be described later.
\begin{Rcode}{fig, scale=.5, height=4, label=oats-intplot}
\coderef{hidden}{par(mar=c(4,4,1,.1))}
with(Oats, interaction.plot(nitro, Variety, yield, type="b"))
\end{Rcode}
\hfill
\begin{Rcode}{fig, !echo, hide, scale=.5, height=4, label=oats-intplot2}
par(mar=c(4,4,1,.1))
library(lme4)
Oats.lmer = lmer(yield ~ Variety + factor(nitro) + (1 | Block/Variety), data=Oats)
Oats.lsms = lsmeans(Oats.lmer, ~ nitro:Variety)
with(Oats.lsms[[1]], interaction.plot(nitro, Variety, lsmean, type="b", ylab="Least-Squares means"))
\end{Rcode}
\\
There is not much evidence of an interaction. In this dataset, we have random factors "Block" and "Block:Variety" (which identifies the plots). So we will fit a linear mixed-effects model that accounts for these. Another technicality is that "nitro" is a numeric variable, and initially we will model it as a factor. We will use "lmer" in the "lme4" package to fit a model.
\enlargethispage{-36pt} %%% MANUAL FORMATTING %%%
\begin{Rcode}
library(lme4, quietly = TRUE, warn.conflicts = FALSE)
Oats.lmer = lmer(yield ~ Variety + factor(nitro) + (1 | Block/Variety), data=Oats)
lsmeans(Oats.lmer, list(revpairwise ~ Variety,  poly ~ nitro,  ~ Variety:nitro))
\end{Rcode}
Unlike the warpbreaks example, the additive model makes it reasonable to look at the marginal lsmeans, which are equally-weighted marginal averages of the cell predictions in the fifth table of the output.\footnote{%
Interestingly, SAS's implementation of least-squares means will refuse to output these cell predictions unless the interaction term is in the model.}
The right-hand interaction plot above was obtained using the statement
\begin{Rcode}{!eval}
with(Oats.lsms[[5]], interaction.plot(nitro, Variety, lsmean, type="b", 
                                      ylab="Least-Squares means"))
\end{Rcode}



While the default for obtaining marginal lsmeans is to weight the predictions equally, we may override this via the "fac.reduce" argument. For example, suppose that we want the "Variety" predictions when "nitro" is 0.25. We can obtain these by interpolation as follows:
\begin{Rcode}
lsmeans(Oats.lmer, ~ Variety, fac.reduce = function(X, lev) .75 * X[2, ] + .25 * X[3, ])
\end{Rcode}
(There is also a "cov.reduce" argument to change the default handling of covariates.)
The polynomial contrasts for "nitro" suggest that we could substitute a quadratic trend for "nitro"; and if we do that, then there is another (probably better) way to make the above predictions:
\begin{Rcode}
OatsPoly.lmer = lmer(yield ~ Variety + poly(nitro, 2) + (1 | Block/Variety), data=Oats)
lsmeans(OatsPoly.lmer, ~ Variety, at = list(nitro = .25))
\end{Rcode}
These predictions are slightly higher than the interpolations mostly because they account for the downward concavity of the fitted quadratics.




\ifx %******************* EXCISED IN FAVOR OF MESSY DATA SECTION ***************************
\section{Rank deficiencies}
In many fitted models, predictions may be made at any factor combination. However, some models have rank deficiencies due to collinearity among predictors or missing factor combinations, and this can mess-up predictions. Consider the following example using a simplified model on a subset of the Oats data.
\begin{Rcode}
just.some = c(1,5,9,11,12,14,15,18,19,20,26,27,29,31,32,33,36)
wildOats.lm = lm(yield ~ Variety*factor(nitro), data=Oats, subset=just.some)
\end{Rcode}
Now consider predictions at various factor combination. Let's first do this manually, using two different parameterizations of the model. 
\begin{Rcode}
illus = data.frame(Variety=levels(Oats$Variety), nitro=0.6)
illus$lsm1 = predict(wildOats.lm, newdata=illus)
# Another parameterization...
wildOats.lm2 = update(wildOats.lm, . ~ Variety*ordered(nitro))
illus$lsm2 = predict(wildOats.lm2, newdata=illus)
illus
\end{Rcode}
We received warnings that there is a rank deficiency. And the predictions obtained illustrate why: they are not the same. In particular, they differ considerably for "Victory" even though they match for the other two varieties.

It happens that the first two predictions match because they are \emph{estimable}, while the third is not. "lsmeans" takes pains to check for estimability, rather than just warning that the predictions may be misleading. Here is the "lsmeans" output for all twelve factor combinations:
\begin{Rcode}
lsmeans(wildOats.lm, ~ Variety*nitro)
\end{Rcode}
The results reflect the fact that two lsmeans are non-estimable due to empty cells.

Here's a more subtle example. Suppose we add a covariate to the "typing" dataset used earlier, and fit a model with two covariates:
\begin{Rcode}
typing$foo = c(63,99,66,33,45,87,81,60,51,51,48,36)
typing.lmfoo = lm(pain ~ hours + foo + type, data=typing)
lsmeans(typing.lmfoo, ~ type)
\end{Rcode}
Everything looks fine, but now plug-in a different value for "hours":
\begin{Rcode}
lsmeans(typing.lmfoo, pairwise ~ type, at = list(hours=65))
\end{Rcode}
None of the lsmeans are estimable now (though, interestingly, the pairwise differences still are). This is due to linear dependence between "hours" and "foo". If you know the linear dependence, you can make predictions:
\begin{Rcode}
lsmeans(typing.lmfoo, ~ type, at = list(hours=65, foo=3*(65-39)))
\end{Rcode}
At the time of this writing, only "lm" and its relatives can support rank deficiencies. 

\fi %************** END OF EXCISED SECTION ************************************



\section{Messy data}
To illustrate some more issues, and related \code{lsmeans} capabilities, consider the dataset named \code{nutrition} that is provided with the \code{lsmeans} package. These data come from Milliken and Johnson (1984), and contain the results of an observational study on nutrition education. Low-income mothers are classified by race, age category, and whether or not they received food stamps (the \code{group} factor); and the response variable is a gain score (post minus pre scores) after completing a nutrition training program. The graph below displays the data.
\begin{Rcode}{fig, width=9in, scale=.5,label=nutr-scatter}
xyplot(gain ~ age | race*group, data=nutrition)
\end{Rcode}

Consider the model that includes all main effects and two-way interactions; and let us look at the \code{group} by \code{race} lsmeans:
\begin{Rcode}
nutr.lm = lm(gain ~ (age + group + race)^2, data = nutrition)
lsmeans(nutr.lm, ~ group*race)
\end{Rcode}
One thing that this illustrates is that \code{lsmeans} incorporates an estimability check, and returns a missing value when a prediction cannot be made uniquely. In this example, we have very few Hispanic mothers in the dataset, resulting in empty cells. This creates a rank deficiency in the fitted model and some predictors are thrown out.

One capability of \code{lsmeans} is that if a factor is included in the \code{at} argument, computations of lsmeans are restricted to the specified level(s). So if we confine ourselves to age group~3, we don't have an estimability issue:
\begin{Rcode}
lsmeans(nutr.lm, ~ group*race, at = list(age = "3"))
\end{Rcode}
Nonetheless, the standard errors for the Hispanic mothers are enormous due to very small counts.
One useful summary of the results is to narrow the scope to two races and the two middle age groups, where most of the data lie. Here are the lsmeans and comparisons within rows and columns
\begin{Rcode}
nutr.lsm = lsmeans(nutr.lm, list(pairwise~group|race, pairwise~race|group),
        at = list(age=c("2","3"), race=c("Black","White")))
nutr.lsm[-3]
\end{Rcode}
An interaction plot of the results follows:
\begin{Rcode}{fig, height=4.5in, scale=.5,label=nutr-intplot}
\coderef{hidden}{par(mar=.1+c(4,4,.5,0))}
with(nutr.lsm[[1]], interaction.plot(group, race, lsmean, type = "b",
    ylab = "Least-squares mean"))
\end{Rcode}
\\
The general conclusion from both is that the expected gains from the training are higher among families receiving food stamps
Note that this analysis is somewhat different than the results we would obtain by subsetting the data, as we are borrowing information from the other observations in estimating and testing these lsmeans.


%\clearpage %-------- MANUAL FORMATTING ---------
\section{GLMM example}
The dataset "cbpp" in the "lme4" package, originally from Lesnoff \emph{et al.}~(1964), provides data on the incidence of contagious bovine pleuropneumonia in 15 herds of zebu cattle in Ethiopia, collected over four time periods. These data are used as the primary example for the "glmer" function, and it is found that a model that accounts for overdisperion is advantageous; hence the addition of the "(1|obs)" in the model fitted below. 

"lsmeans" may be used as in linear models to obtain marginal linear predictions for a generalized linear model or, in this case, a generalized linear mixed model. Here, we use the "trt.vs.ctrl1" contrast family to compare each period with the first, as the primary goal was to track the spread or decline of CBPP over time.
We will save the results from "lsmean", then add the inverse logits of the predictions and the estimated odds ratios for the comparisons as an aid in interpretation.
\begin{Rcode}
cbpp$obs = 1:nrow(cbpp)
cbpp.glmer = glmer(cbind(incidence, size - incidence) 
   ~ period + (1 | herd) + (1 | obs),  family = binomial,  data = cbpp)
\end{Rcode}
\begin{Rcode}
cbpp.lsm = lsmeans(cbpp.glmer, trt.vs.ctrl1 ~ period)
cbpp.lsm[[1]]$pred.incidence = 1 - 1 / (1 + exp(cbpp.lsm[[1]]$lsmean))
cbpp.lsm[[2]]$odds.ratio = exp(cbpp.lsm[[2]]$estimate)
cbpp.lsm
\end{Rcode}
When degrees of freedom are not available, as in this case, "lsmeans" emphasizes that fact by displaying "NA" for degrees of freedom and in the column headings.
%In a way, the comparisons table is not needed because the results are the same as the regression coefficients under the default parameterization.

\section{Contrasts}
You may occasionally want to know exactly what contrast coefficients are being used, especially in the polynomial case. Contrasts are implemented in functions having names of the form \texttt{\textsl{name}.lsmc} (``lsmc'' for ``least-squares means contrasts''), and you can simply call that function to see the contrasts; for example,
\begin{Rcode}
poly.lsmc(1:4)
\end{Rcode}
"poly.lsmc" uses the base function "poly" plus an \emph{ad hoc} algorithm that tries (and usually succeeds) to make integer coefficients, copmparable to what you find in published tables of orthogonal polynomial contrasts.

You may supply your own custom contrasts in two ways. One is to supply a "contr" argument in the "lsmeans" call, like this:
\begin{Rcode}
print(lsmeans(typing.lm, custom.comp ~ type, 
              contr = list(custom.comp = list(A.vs.others=c(1, -.5, -.5)))), 
    omit=1)
\end{Rcode}
Each contrast family is potentially a list of several contrasts, and there are potentially more than one contrast family; so we must provide a list of lists.

The other way is to create your own ".lsmc" function, and use its base name in a formula:
\begin{Rcode}
inward.lsmc = function(levs, ...) {
    n = length(levs)
    result = data.frame(`grand mean` = rep(1/n, n))
    for (i in 1 : floor(n/2)) {
        x = rep(0, n)
        x[1:i] = 1/i
        x[(n-i+1):n]  = -1/i
        result[[paste("first", i, "vs last", i)]] = x
    }
    attr(result, "desc") = "grand mean and inward contrasts"
    attr(result, "adjust") = "none"
    result
}
\end{Rcode}
Testing it, we have
\begin{Rcode}
inward.lsmc(1:5)
\end{Rcode}
\ldots\ and an application:
\begin{Rcode}
print(lsmeans(Oats.lmer, inward ~ nitro), omit=1)
\end{Rcode}




\section*{References}
\begin{description}

\item[Box, G., Hunter,, J., and Hunter, W.~(2005)]
\emph{Statistics for Experimenters} (2nd ed.).
Wiley.

\item[Harvey, W.~(1976)] Use of the HARVEY procedure.
SUGI Proceedings, 
\url{http://www.sascommunity.org/sugi/SUGI76/Sugi-76-20%20Harvey.pdf} (accessed June 20, 2013).

\item[Lesnoff, M., Laval, G., Bonnet, P., \emph{et al.}~(2004)] Within-herd spread of contagious bovine pleuropneumonia in Ethiopian highlands, \emph{Preventive Veterinary Medicine},  \textbf{64}, 27--40.

\item[Milliken, G. A. and Johnson, D. E. (1984)]
\emph{Analysis of Messy Data -- Volume I: Designed Experiments}. Van Nostrand, ISBN 0-534-02713-7.


\item[Oehlert, G. (2000)] \emph{A First Course in Design and Analysis of Experiments}, W.~H.~Freeman.
This is out-of-print, but now available under a Creative Commons license via \url{http://users.stat.umn.edu/~gary/Book.html} (accessed August 23, 2012).

\item[SAS Institute Inc. (2012)]
Online documentation, SAS/STAT version 9.3: Shared concepts: LSMEANS statement.
\url{http://support.sas.com/documentation/cdl/en/statug/63962/HTML/default/viewer.htm#statug_introcom_a0000003362.htm} (accessed August 14, 2012).

\item[Tukey, J.~W.~(1977)] \emph{Exploratory Data Analysis}. Addison-Wesley. 

\item[Yates, F. (1935)] Complex experiments, \emph{Journal of the Royal Statistical Society} (Supplement), \textbf{2}, 181--247.

\end{description}


\end{document}


