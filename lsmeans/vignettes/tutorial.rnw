\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{mathpazo}
\usepackage{fancyvrb}
\usepackage{natbib}
\usepackage{hyperref}
%\usepackage{multicol}

\let\dq="
\DefineShortVerb{\"}

\def\pkg#1{\textsf{#1}}
\def\lsm{\pkg{lsmeans}}
\def\code{\texttt}
\def\proglang{\textsf}

% double-quoted text
\def\dqt#1{\code{\dq{}#1\dq{}}}

% The objects I want to talk about
\def\rg{\dqt{ref.grid}}
\def\lsmo{\dqt{lsmobj}}

\def\R{\proglang{R}}
\def\SAS{\proglang{SAS}}


\def\Fig#1{Figure~\ref{#1}}
\def\bottomfraction{.5}

\title{\lsm{} tutorial}
\author{Russell V.~Lenth}

%\VignetteIndexEntry{lsmeans tutorial}
%\VignetteDepends{lsmeans}
%\VignetteKeywords{least-squares means}
%\VignettePackage{lsmeans}


% Initialization
<<echo=FALSE>>=
options(show.signif.stars=FALSE, prompt="R> ", continue="   ")
@



\begin{document}
\SweaveOpts{concordance=TRUE}
\maketitle{}

\section{Introduction}
Least-squares means (LS~means for short)  for a linear model are simply predictions---or marginal averages thereof---over a regular grid of predictor settings which I call the \emph{reference grid}. They date back at least to 1976 when LS~means were incorporated in the contributed \SAS{} procedure named \code{HARVEY} \citep{Har76}. Later, they were incorporated via \code{LSMEANS} statements in the regular \SAS{} releases. 

In simple analysis-of-covariance models, LS~means are the same as covariate-adjusted means. In unbalanced factorial experiments, LS~means for each factor mimic the marginal means but are adjusted to bias due to imbalance. The latter interpretation is quite similar to the ``unweighted means'' method for unbalanced data, as presented in old design books.

In any case, the most important things to remember are:
\begin{itemize}
\item LS~means are computed relative to a \emph{reference grid}.
\item Once the reference grid is understood, LS~means are simply predictions on this grid, or marginal averages of these predictions.
\end{itemize}
If you understand these points, then you will know what you are getting, and can judge whether or not LS~means are appropriate for your analysis.

\section{The reference grid}
Since the reference grid is fundamental, it is our starting point. For each predictor in the model, we define a set of one or more \emph{reference levels}. The reference grid is then the set of all combinations of reference levels. If not specified explicitly, the default reference levels are obtained as follows:
\begin{itemize}
\item For each predictor that is a factor, its reference levels are the unique levels of that factor.
\item Each numeric predictor has just one reference level---its mean over the dataset. 
\end{itemize}
So the reference grid depends on both the model and the dataset.

\subsection{Example: Orange sales}
To illustrate, consider the "oranges" data provided with \lsm{}.  This dataset has sales of two varieties of oranges (response variables "sales1" and "sales2") at 6 stores (factor "store"), over a period of 6 days (factor "day"). The prices of the oranges (covariates "price1" and "price2") fluctuate in the different stores and the different days. There is just one observation on each store on each day.

For starters, let's consider an additive covariance model for sales of the first variety, with the two factors and both "price1" and "price2" as covariates (since the price of the other variety could also affect sales).
<<>>=
library(lsmeans)
oranges.lm1 = lm(sales1 ~ price1 + price2 + day + store, data = oranges)
anova(oranges.lm1)
@
The "ref.grid" function in \lsm{} may be used to establish the reference grid. Here is the default one:
<<>>=
( oranges.rg1 = ref.grid(oranges.lm1) )
@
As outlined above, the two covariates "price1" and "price2" have their means as their sole reference level; and the two factors have their levels as reference levels. The reference grid thus consists of the $1\times1\times6\times6=36$ combinations of these reference levels. LS~means are based on predictions on this reference grid, which we can obtain using "predict" or "summary":
<<>>=
summary(oranges.rg1)
@
The ANOVA indicates there is a significant "day" effect after adjusting for the covariates, so we might want to compare the days. The "lsmeans" function can do this:
<<>>=
lsmeans(oranges.rg1, "day")   ## or lsmeans(oranges.lm1, "day")
@
These results, as indicated in the annotation in the output, are in fact the averages of the predictions shown earlier, for each day, over the 6 stores. The above LS~means are not the same as the marginal means of the data:
<<>>=
with(oranges, tapply(sales1, day, mean))
@
These unadjusted means are biased by having different "price1" and "price2" values on each day, whereas the LS~means adjust for bias by using predictions at uniform "price1" and "price2" values.

Note that you may call "lsmeans" with either the reference grid or the model. If the model is given, then the first thing it does is create the reference grid; so if you already have the reference grid, as in this example, it's more efficient to make use of it.

\subsection{Altering the reference grid}
The "at" argument may be used to override defaults in the reference grid. You may specify this argument either in a "ref.grid" call or an "lsmeans" call; and you should specify "list" with named sets of reference levels. Here is a silly example:
<<>>=
lsmeans(oranges.lm1, "day", at = list(price1 = 50, 
    price2 = c(40,60), day = c("2","3","4")) )
@
Here, we restricted the results to three of the days, and used different prices.
One possible surprise is to note that the predictions are averaged over the two "price2"
values. That is because "price2" is no longer a single reference level, and we average over the levels of all factors not used to split-out the LS~means. This is probably not what we want. To get separate sets of predictions for each "price2", you need to specify it as another factor or as a "by" factor in the "lsmeans" call (we will save the result for later discussion):
<<>>=
org.lsm = lsmeans(oranges.lm1, "day", by ="price2", 
    at = list(price1 = 50, price2 = c(40,60), day = c("2","3","4")) )
org.lsm
@
Note: We could have obtained the same results using any of these:
<<eval=FALSE>>=
lsmeans(oranges.lm1, ~ day | price, at = ... )         # Ex 1
lsmeans(oranges.lm1, c("day","price2"), at = ... )     # Ex 2
lsmeans(oranges.lm1, ~ day * price, at = ... )         # Ex 3
@
Ex~1 illustrates the formula method for specifying factors, which is more compact. The "|" character replaces the "by" specification. Ex~2 and Ex~3 produce the same results, but their results are displayed as one table (with columns for "day" and "price") rather than as two separate tables.

\section{Working with the results}
The "ref.grid" function produces an object of class \rg{}, and the "lsmeans" function produces an object of class \lsmo{}, which is a subclass of \rg. There is really no practical difference between these two classes except for their "show" methods---what is displayed if you just call the functions---and the fact that an \lsmo{} is not (necessarily) a true reference grid as defined earlier in this tutorial. Let's use the "str" function to examine the \lsmo{} object produced just above:
<<>>=
str(org.lsm)
@
We no longer see the reference levels for all predictors in the model---only the levels of "day" and "price2". These \emph{act} like reference levels, but they do not define the reference grid upon which the predictions are based.

There are several methods for \rg{} (and hence also for \lsmo{}) objects. One you have seen already is "summary". It has a number of arguments: see its help page. In the following call, we summarize "days.lsm" differently than before. We will also save the object produced by "summary" for further discussion.
<<>>=
( org.sum = summary(org.lsm, infer=c(TRUE,TRUE), 
                    level=.90, adjust="bon", by = "day") )
@
The "infer" argument caused both confidence intervals and tests to be produced. The default confidence level of $.95$ was overridden; a Bonferroni adjustment was applied to both the intervals and the $P$~values; and the tables are organized the opposite way from what we saw before.

What kind of object was produced by "summary"? Let's see:
<<>>=
class(org.sum)
@
The \dqt{summary.ref.grid} class is an extension of \dqt{data.frame}. It includes some attributes that, among other things, cause the messages seen in the example to appear when the object is displayed. But it can also be used as a \dqt{data.frame} if you just want to use the results computationally. For example, suppose we want to convert the LS~means from dollars to Russian rubles:
<<>>=
cbind(org.sum[, 1:2], lsrubles = org.sum$lsmean * 35.7) # as of April 22, 2014
@
Observe that, as a data~frame, the summary is just one table with six rows, rather than a collection of three tables, and it contains a column for all reference variables, including any "by" variables.

Besides "str" and "summary", there is also a "confint" method (same is "summary" with "infer=c(TRUE,FALSE)") and a "test" method (same as "summary" with "infer=c(FALSE,TRUE)"). There is also an "update" method which may be used for changing the default display settings. For example:
<<>>=
org.lsm2 = update(org.lsm, by.vars = NULL, level = .99)
org.lsm2
@

\section{Contrasts and comparisons}
Often, people want to do pairwise comparisons of LS~means, or compute other contrasts among them. This is the purpose of the "contrast" function, which uses an \dqt{lsmobj} object as input. There are several standard contrast families such as \dqt{pairwise}, \dqt{trt.vs.ctrl}, and \dqt{poly}. In the following command, we request \dqt{eff} contrasts, which are differences between each mean and the overall mean:
<<>>=
contrast(org.lsm, "eff")
@
Note that this remembers the "by" specification from before, and obtains the effects for each group. In this example, since it is an additive model, we obtain the same results in each group. This isn't wrong, it's just redundant.

Another popular method is Dunnett-style contrasts, where a particular LS~mean is compared with each of the others. This is done using \dqt{trt.vs.ctrl}. In the following, we obtain (again) the LS~means for days, and compare each with the average of the LS~means on day~5 and~6.
<<>>=
days.lsm = lsmeans(oranges.rg1, "day")
contrast(days.lsm, "trt.vs.ctrl", ref = c(5,6))
@
For convenience, \dqt{trt.vs.ctrl1} or \dqt{trt.vs.ctrlk} methods are provided for use in lieu of "ref" for comparing with the first and the last LS~means.

You may have noticed that by default, "lsmeans" results are displayed with confidence intervals while "contrast" results are displayed with $t$ tests. You can easily override this; for example,
<<eval=FALSE>>=
confint(contrast(days.lsm, "trt.vs.ctrlk"))
@
(Results not shown.)

In the above examples, a default multiplicity adjustment is determined from the contrast method. This may be overridden by additing an "adjust" argument. 

\subsection{Pairwise comparisons}
Often, users want pairwise comparisons among the LS~means. These may be obtained by specifying \dqt{pairwise} or \dqt{revpairwise} in the call to "contrast". For group labels $A,B,C$, \dqt{pairwise} generates the comparisons $A-B, A-C, B-C$ while \dqt{revpairwise} generates $B-A, C-A, C-B$. As a convenience, the "pairs" method works just like "contrasts" with \dqt{pairwise}:
<<>>=
pairs(org.lsm)
@
There is also a "cld" (compact letter display) method that lists the LS~means along with grouping symbols for pairwise contrasts. It requires the \pkg{multcompView} package to be installed.
<<>>=
cld(days.lsm, alpha = .10)
@
Two LS~means that share one or more of the same grouping symbols are not significantly different at the stated value of "alpha", after applying the multiplicity adjustment (in this case Tukey's HSD).
By default, the LS~means are ordered in this display, but this may be overridden with the argument "sort=FALSE". "cld" returns a \dqt{summary.ref.grid} object, not an "lsmobj".

\section{Multivariate models}
The "oranges" data has two response variables. Let's try a multivariate model for predicting the sales of the two varieties of oranges, and see what we get if we call "ref.grid":
<<>>=
oranges.mlm = lm(cbind(sales1,sales2) ~ price1 + price2 + day + store, 
                 data = oranges)
ref.grid(oranges.mlm)
@
What happens is that the multivariate response is treated like an additional factor, by default named "rep.meas". In turn, it can be used, to specify levels for LS~means. Here we rename the multivariate response to \dqt{variety} and obtain "day" means (and a compact letter display for comparisons thereof)  for each "variety":
<<>>=
org.mlsm = lsmeans(oranges.mlm, ~ day | variety, mult.name="variety")
cld(org.mlsm, sort = FALSE)
@

\subsection{Contrasts of contrasts}
With the preceding model, we might want to compare the two varieties on each day:
<<>>=
org.vardiff = pairs(org.mlsm, by = "day")
@
The results (not yet shown) will comprise the six "sales1-sales2" differences, one for each day. We might want to compare these differences to see if they vary from day to day.
<<>>=
cld(org.vardiff, by = NULL)
@


\section{Interfacing with \pkg{multcomp}}
The \pkg{multcomp} package supports more exacting corrections for simultaneous inference than are available in \lsm{}. Its "glht" (general linear hypothesis testing) function and associated \dqt{glht} class are similar in some ways to "lsmeans" and \dqt{lsmobj} objects, respectively. So we provide methods such as "as.glht" for working with "glht" so as to obtain ``exact'' inferences. To illustrate, I'll compare some simultaneous confidence intervals using the two packages. 
%%%\begin{multicols}{2}\small
First, using a Bonferroni correction on the LS~means for "day" in the "oranges" model:
<<>>=
confint(days.lsm, adjust = "bon")
@
%%%\columnbreak
And now using \pkg{multcomp}:
<<>>=
library(multcomp)
confint(as.glht(days.lsm))
@
%%%\end{multicols}
The latter intervals are somewhat narrower, which is expected since the Bonferroni bethod is conservative.

The \lsm{} package also provides an "lsm" function that can be called as the second argument of "glht":
<<>>=
summary(glht(oranges.lm1, lsm("day", contr="eff")), test = adjusted("free"))
@





\section{Example: Oat yields}
The "Oats" dataset in the \pkg{nlme} has the results of a split-plot experiment. The experiment was conducted on six blocks (factor "Block"). Each block was divided into three plots, which were randomized to three varieties (factor "Variety") of oats. Each plot was divided into subplots and randomized to four levels of nitrogen (variable "nitro"). The response, "yield", was measured once on each subplot after a suitable growing period. 

We will fit a model using the "lmer" function in the \pkg{lme4} package. This will be a mixed model with random intercepts for "Block" and "Block:Variety" (which identifies the plots). I have elected to apply a logarithmic transformation to the response variable, mostly for illustration purposes.
<<>>=
data("Oats", package = "nlme")
library("lme4")
Oats.lmer = lmer(log(yield) ~ Variety*factor(nitro) + (1|Block/Variety), 
                 data = Oats)
anova(Oats.lmer)
@


\bibliography{lsmeans}\bibliographystyle{jss}

\end{document}
